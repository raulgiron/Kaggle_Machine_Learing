{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7664352b-f40b-434f-b8f8-cf4e0da961ca",
   "metadata": {},
   "source": [
    "# INTRO TO MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50be5cdb-4e81-452d-b01f-2d0e8b1f7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62848e6-fa8a-4dd9-903a-2af4aa0b5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_with_commas(x):\n",
    "    return '{:,.2f}'.format(x)\n",
    "\n",
    "pd.options.display.float_format = format_with_commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154a50d2-65b0-4208-9613-8c70920ce4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_file_path = '/Volumes/Memory/kaggle/machine_learning/melb_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b4dab9-8c2d-46fc-a90b-c72141cfc6ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Memory/kaggle/machine_learning/melb_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m melbourne_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmelbourne_file_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\kaggle\\machine_learning\\ml-venv-windows\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\kaggle\\machine_learning\\ml-venv-windows\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\kaggle\\machine_learning\\ml-venv-windows\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\kaggle\\machine_learning\\ml-venv-windows\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\kaggle\\machine_learning\\ml-venv-windows\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Memory/kaggle/machine_learning/melb_data.csv'"
     ]
    }
   ],
   "source": [
    "melbourne_data = pd.read_csv(melbourne_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377679a-9bbc-44b0-aeb1-8d0b19724570",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a07926-12bc-48f9-b5f8-328d0e3aeb98",
   "metadata": {},
   "source": [
    "### Print the list of columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849d81b-1bc7-4d2a-b397-433e362d013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d1c2f-2196-4e9b-98df-e4a5d053e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data = melbourne_data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6c1ac-db2b-40e9-b7fc-dace65a3ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e285ff3-0fca-4b74-b59e-e264616f4455",
   "metadata": {},
   "source": [
    "# Selecting The Prediction Target of the Model - By convention, the prediction target is called \" y \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3436ad-f60e-4517-8309-26de211e04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = melbourne_data.Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97ebdf-8101-4437-9e8b-291e7a4747d3",
   "metadata": {},
   "source": [
    "# Choosing the \"Features\" of the Model - By convention, this data is called \" X \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6dcbf-788e-4205-8cc3-e3092731e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c7a54-cf78-4532-839f-1a05f9a9bc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = melbourne_data[melbourne_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23c515-c5c9-47e7-a159-bc7bc906d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c466a51-6c75-4f2c-9404-8c837f3be43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f3707-c338-4805-961b-6f6f06e2a92d",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391a4c3c-c648-427e-9843-e9f93d3858e7",
   "metadata": {},
   "source": [
    "# Building the Model with Scikit-Learn Python Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9610fa52-51a0-4503-88d8-f2b4bf7fbb9e",
   "metadata": {},
   "source": [
    "When coding, this library is written as sklearn\n",
    "\n",
    "### The steps to building and using a model are:\n",
    "\n",
    "Define: What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.\n",
    "\n",
    "Fit: Capture patterns from provided data. This is the heart of modeling.\n",
    "\n",
    "Predict: Just what it sounds like\n",
    "\n",
    "Evaluate: Determine how accurate the model's predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45a670-3203-47a3-971d-f16d6cd621c2",
   "metadata": {},
   "source": [
    "# Define and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069081c-3b11-4dd4-9f2e-b1e91ebcdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "melbourne_model = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "melbourne_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5ed40-a470-4e11-9348-ef0f86284ad4",
   "metadata": {},
   "source": [
    "### Making a Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f802c-71a1-4cc1-8ce7-8014b032af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Making predictions for the following 5 houses:\")\n",
    "print(X.head())\n",
    "print(\"The predictions are\")\n",
    "print(melbourne_model.predict(X.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fd941-5452-4202-8ca1-9cf3fa52a8dd",
   "metadata": {},
   "source": [
    "### Calculating the Mean Absolute Error MAE for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679a9ea-2674-4cef-9433-e4d95301f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)\n",
    "print(mean_absolute_error(y, predicted_home_prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82756c-0773-428d-bef4-2081c94d6cb8",
   "metadata": {},
   "source": [
    "### Split the data into two - one for training the model and another for validating the model MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf1668-f6e4-4886-a50f-05454e089c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "# Define the model\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "# Fit the model\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "# Get predicted price on validation (val) data\n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18996589-9a41-4a24-b2d4-934db492e4ba",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a4af4-8a1b-48ba-aae3-31569e596a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, tain_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f132485-9e8d-47f1-ae3b-fa9ae1415a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melbourne_file_path = '/Volumes/Memory/kaggle/machine_learning/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "y = filtered_melbourne_data.Price\n",
    "X = filtered_melbourne_data[melbourne_features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaf190a-ffdc-49ee-bb11-5a22fd0ac73e",
   "metadata": {},
   "source": [
    "### Compare the MAE with differing values of max_leaf_nodes through a for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a18e8b-68ec-472d-b2c4-70612d2e20bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaf_nodes_candidates = [5, 50, 500, 5000]\n",
    "mae_values = []\n",
    "for max_leaf_nodes in max_leaf_nodes_candidates:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    mae_values.append(my_mae)\n",
    "    print(\"Max leaf nodes: %d \\t\\t Mean Absolute Error: %d\" %(max_leaf_nodes, my_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f177a-4d04-4e04-a1d1-cac626293cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "best_mae_index = np.argmin(mae_values)\n",
    "best_mae_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1f7ff1-15ca-4626-8fcb-dbdaaa5a86c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_size = max_leaf_nodes_candidates[best_mae_index]\n",
    "best_tree_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c796af7-c88e-4e1b-880d-bc239811d131",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3cfa89-f00e-47e7-aba1-55fcfa000522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def format_with_commas(x):\n",
    "    return '{:,.2f}'.format(x)\n",
    "pd.options.display.float_format = format_with_commas\n",
    "\n",
    "melbourne_file_path = '/Volumes/Memory/kaggle/machine_learning/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melbourne_file_path)\n",
    "\n",
    "filtered_melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "melbourne_features = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude']\n",
    "y = filtered_melbourne_data.Price\n",
    "X = filtered_melbourne_data[melbourne_features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a364ffd-13be-4284-bcea-60ae3ec4bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_forest_mae(train_X, val_X, tain_y, val_y):\n",
    "    forest_model = RandomForestRegressor(random_state=1)\n",
    "    forest_model.fit(train_X, train_y)\n",
    "    melb_preds = forest_model.predict(val_X)\n",
    "    forest_mae = mean_absolute_error(val_y, melb_preds)\n",
    "    return(forest_mae)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f8722-4940-4a21-a695-2865dad1b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_forest_mae(train_X, val_X, train_y, val_y)\n",
    "result_formatted = format_with_commas(result)\n",
    "print(result_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbcce5-9f14-49c5-86b4-fa4682337559",
   "metadata": {},
   "source": [
    "Intro to Machine Learning Course ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1cc46-e88e-4c6e-83db-f7e6ec7c0f76",
   "metadata": {},
   "source": [
    "# INTERMEDIATE MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cde5fa-a9c0-4a88-9968-c2888e12745f",
   "metadata": {},
   "source": [
    "## Dealing with missing values (Three Approaches)\n",
    "Drop Columns with Missing Values, Imputation, An Extension To Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a34b87f-b737-419e-b64a-a98c75eb575b",
   "metadata": {},
   "source": [
    "### 1-) A Simple Option: Drop Columns with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0daf34-bb59-43ea-8834-07fed71ffd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('/Volumes/Memory/kaggle/machine_learning/melb_data.csv')\n",
    "y = data.Price\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "melb_predictors = data.drop(['Price'], axis = 1)\n",
    "X = melb_predictors.select_dtypes(exclude = ['object'])\n",
    "# Divide data into training and validation subsets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73dc30d-a38c-498f-a77f-7d632e0ee689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "\n",
    "# Numbers of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc237b9d-5bcf-4887-99ec-b5b09a48ad78",
   "metadata": {},
   "source": [
    "### Define Function to Measure Quality of Each Approach\n",
    "We define a function score_dataset() to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcec89-3f14-4460-9021-1676c94ce02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170c105-8703-4402-a6c3-457aaec9e31d",
   "metadata": {},
   "source": [
    "### Score from Approach 1 (Drop Columns with Missing Values)\n",
    "Since we are working with both training and validation sets, we are careful to drop the same columns in both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ab7b6-cc44-486e-89a8-5d4a44140a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis = 1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5c5f9-cb2d-4fc6-be2d-5f369007cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 1 (Drop columns  with missing values):\")\n",
    "mae = score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab2f04-fc86-4b49-8a12-c0a8efff13b9",
   "metadata": {},
   "source": [
    "### 2-) A Better Option: Imputation\n",
    "Next, we use \"SimpleImputer\" to replace missing values with the mean value along each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b35a3c-2937-47d0-8140-6754fe6cddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb1478-72c6-4b45-8c38-68f0ef9b673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 2 (Imputation):\")\n",
    "mae = score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0fbc2-e8b0-4395-9850-062adaf85720",
   "metadata": {},
   "source": [
    "We see that Approach 2 has lower MAE than Approach 1, so Approach 2 performed better on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431c945c-b939-4f91-9f77-2ac0914c05eb",
   "metadata": {},
   "source": [
    "### 3-) An Extension To Imputation\n",
    "\n",
    "Next, we impute the missing values, while also keeping track of which values were imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ba843-38c8-48a2-bd69-0e1ae544f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92fd00-2a77-47ce-b4f6-ed23fbd25b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 3 (An Extension to Imputation):\")\n",
    "mae = score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bccb427-381f-435e-8f08-9cb91157f7d3",
   "metadata": {},
   "source": [
    "## Categorical Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62593c48-b90a-43b0-9ec2-dff7b8888042",
   "metadata": {},
   "source": [
    "## Dealing with categorical variables (Three Approaches)\n",
    "Drop Categorical Variables, Ordinal Encoding, One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2849c3-c569-4ecf-9ac7-24c7463d8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "data = pd.read_csv('melb_data.csv')\n",
    "\n",
    "# Separate target from predictors\n",
    "y = data.Price\n",
    "X = data.drop(['Price'], axis = 1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
    "X_train_full.drop(cols_with_missing, axis = 1, inplace = True)\n",
    "X_valid_full.drop(cols_with_missing, axis = 1, inplace = True)\n",
    "\n",
    "# Cardinality means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b68f475-a36c-4dd1-a722-472515b3cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17a3ae-2f1d-4c32-8381-44635d4e3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbad266-aa62-4426-84f1-521e4f8a619d",
   "metadata": {},
   "source": [
    "### Define Function to Measure Quality of Each Approach\n",
    "We define a function score_dataset() to compare different approaches to dealing with missing values. This function reports the mean absolute error (MAE) from a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53664b-dfa0-4df9-8b04-0ee9ee8edc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12c85c2-e5c6-4fcf-bea0-e4d2462a9cd6",
   "metadata": {},
   "source": [
    "### 1-) Score from Approach 1 (Drop Categorical Variables)\n",
    "The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70f22ab-e117-4a27-af83-4a6fd622c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude = ['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude = ['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5a62f-4b5f-421f-9318-50e48dcc2725",
   "metadata": {},
   "source": [
    "### Drop the problematic categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87436cb4-afad-4805-bb81-aeaff1dcedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical columns in the training data\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely ordinal encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_valid[col]).issubset(set(X_train[col]))]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be ordinal encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b0cee-af93-46ac-85d4-ce16c36d5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 1 (Drop categorical variables):\")\n",
    "mae = score_dataset(drop_X_train, drop_X_valid, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e19daa1-8b28-4057-8c73-0e5ecac24326",
   "metadata": {},
   "source": [
    "### 2-) Score from Approach 2 (Ordinal Encoding)\n",
    "Ordinal encoding assigns each unique value to a different integer. Scikit-learn has a OrdinalEncoder class that can be used to get ordinal encodings. We loop over the categorical variables and apply the ordinal encoder separately to each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1e903-738a-4e0b-b466-0dbdde75c6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\n",
    "label_X_valid[good_label_cols] = ordinal_encoder.transform(X_valid[good_label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f043f-fd50-4c78-843e-fcba83aabdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_X_train[object_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde77b9-ffa0-4f16-8748-8f520f2c9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 2 (Ordinal Encoding):\")\n",
    "mae = score_dataset(label_X_train, label_X_valid, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5b0a3-19e9-4119-a0fd-daffc50dea9b",
   "metadata": {},
   "source": [
    "### 3-) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ba31f-1f36-4dcc-9299-5d3636516f66",
   "metadata": {},
   "source": [
    "One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.\n",
    "\n",
    "One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).\n",
    "\n",
    "We use the OneHotEncoder class from scikit-learn to get one-hot encodings. There are a number of parameters that can be used to customize its behavior.\n",
    "\n",
    "We set handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data, and setting sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0b15d-5025-42df-a7c7-1db8c9523af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f514c818-d88d-4f22-b8aa-f616b9ba0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE from approach 3 (One-Hot Encoding):\")\n",
    "mae = score_dataset(OH_X_train, OH_X_valid, y_train, y_valid)\n",
    "formatted_mae = f\"{mae:,.2f}\"\n",
    "print(formatted_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12399494-ead2-4ad8-85bf-6d4ace6237c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
